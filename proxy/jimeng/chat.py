#ä½œè€…ï¼šå‡Œå° (å¾®ä¿¡fengin)
#GITHUB: https://github.com/fengin/image-gen-server.git
#ç›¸å…³çŸ¥è¯†å¯ä»¥çœ‹AIå…¨ä¹¦ï¼šhttps://aibook.ren 


"""å¯¹è¯è¡¥å…¨ç›¸å…³åŠŸèƒ½"""

import re
import time
from typing import Dict, List, Optional, Union, Generator
import random

from . import utils
from .images import generate_images, DEFAULT_MODEL
from .exceptions import API_REQUEST_PARAMS_INVALID

MAX_RETRY_COUNT = 3
RETRY_DELAY = 5000

def parse_model(model: str) -> Dict[str, Union[str, int]]:
    """è§£ææ¨¡å‹å‚æ•°
    
    Args:
        model: æ¨¡å‹åç§°
        
    Returns:
        Dict: æ¨¡å‹ä¿¡æ¯
    """
    model_name, size = model.split(':') if ':' in model else (model, None)
    if not size:
        return {
            'model': model_name,
            'width': 1024,
            'height': 1024
        }
        
    match = re.search(r'(\d+)[\W\w](\d+)', size)
    if not match:
        return {
            'model': model_name,
            'width': 1024,
            'height': 1024
        }
        
    width, height = match.groups()
    return {
        'model': model_name,
        'width': int((int(width) + 1) // 2 * 2),  # ç¡®ä¿æ˜¯å¶æ•°
        'height': int((int(height) + 1) // 2 * 2)  # ç¡®ä¿æ˜¯å¶æ•°
    }

async def create_completion(
    messages: List[Dict[str, str]],
    refresh_token: str,
    model: str = DEFAULT_MODEL,
    retry_count: int = 0
) -> Dict:
    """åŒæ­¥å¯¹è¯è¡¥å…¨
    
    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        refresh_token: åˆ·æ–°token
        model: æ¨¡å‹åç§°
        retry_count: é‡è¯•æ¬¡æ•°
        
    Returns:
        Dict: è¡¥å…¨ç»“æœ
        
    Raises:
        API_REQUEST_PARAMS_INVALID: å‚æ•°æ— æ•ˆ
    """
    try:
        if not messages:
            raise API_REQUEST_PARAMS_INVALID("æ¶ˆæ¯ä¸èƒ½ä¸ºç©º")
            
        # è§£ææ¨¡å‹å‚æ•°
        model_info = parse_model(model)
        
        # ç”Ÿæˆå›¾åƒ
        image_urls = generate_images(
            model=model_info['model'],
            prompt=messages[-1]['content'],
            width=model_info['width'],
            height=model_info['height'],
            refresh_token=refresh_token
        )
        
        # æ„é€ è¿”å›ç»“æœ
        return {
            'id': utils.generate_uuid(),
            'model': model or model_info['model'],
            'object': 'chat.completion',
            'choices': [{
                'index': 0,
                'message': {
                    'role': 'assistant',
                    'content': ''.join(f'![image_{i}]({url})\n' for i, url in enumerate(image_urls))
                },
                'finish_reason': 'stop'
            }],
            'usage': {
                'prompt_tokens': 1,
                'completion_tokens': 1,
                'total_tokens': 2
            },
            'created': utils.get_timestamp()
        }
    except Exception as e:
        if retry_count < MAX_RETRY_COUNT:
            print(f"Response error: {str(e)}")
            print(f"Try again after {RETRY_DELAY / 1000}s...")
            time.sleep(RETRY_DELAY / 1000)
            return await create_completion(messages, refresh_token, model, retry_count + 1)
        raise e

async def create_completion_stream(
    messages: List[Dict[str, str]],
    refresh_token: str,
    model: str = DEFAULT_MODEL,
    retry_count: int = 0
) -> Generator[Dict, None, None]:
    """æµå¼å¯¹è¯è¡¥å…¨
    
    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        refresh_token: åˆ·æ–°token
        model: æ¨¡å‹åç§°
        retry_count: é‡è¯•æ¬¡æ•°
        
    Yields:
        Dict: è¡¥å…¨ç»“æœç‰‡æ®µ
    """
    try:
        if not messages:
            yield {
                'id': utils.generate_uuid(),
                'model': model,
                'object': 'chat.completion.chunk',
                'choices': [{
                    'index': 0,
                    'delta': {'role': 'assistant', 'content': 'æ¶ˆæ¯ä¸ºç©º'},
                    'finish_reason': 'stop'
                }]
            }
            return
            
        # è§£ææ¨¡å‹å‚æ•°
        model_info = parse_model(model)
        
        # å‘é€å¼€å§‹ç”Ÿæˆæ¶ˆæ¯
        yield {
            'id': utils.generate_uuid(),
            'model': model or model_info['model'],
            'object': 'chat.completion.chunk',
            'choices': [{
                'index': 0,
                'delta': {'role': 'assistant', 'content': 'ğŸ¨ å›¾åƒç”Ÿæˆä¸­ï¼Œè¯·ç¨å€™...'},
                'finish_reason': None
            }]
        }
        
        try:
            # ç”Ÿæˆå›¾åƒ
            image_urls = generate_images(
                model=model_info['model'],
                prompt=messages[-1]['content'],
                width=model_info['width'],
                height=model_info['height'],
                refresh_token=refresh_token
            )
            
            # å‘é€å›¾åƒURL
            for i, url in enumerate(image_urls):
                yield {
                    'id': utils.generate_uuid(),
                    'model': model or model_info['model'],
                    'object': 'chat.completion.chunk',
                    'choices': [{
                        'index': i + 1,
                        'delta': {
                            'role': 'assistant',
                            'content': f'![image_{i}]({url})\n'
                        },
                        'finish_reason': None if i < len(image_urls) - 1 else 'stop'
                    }]
                }
                
            # å‘é€å®Œæˆæ¶ˆæ¯
            yield {
                'id': utils.generate_uuid(),
                'model': model or model_info['model'],
                'object': 'chat.completion.chunk',
                'choices': [{
                    'index': len(image_urls) + 1,
                    'delta': {
                        'role': 'assistant',
                        'content': 'å›¾åƒç”Ÿæˆå®Œæˆï¼'
                    },
                    'finish_reason': 'stop'
                }]
            }
                
        except Exception as e:
            # å‘é€é”™è¯¯æ¶ˆæ¯
            yield {
                'id': utils.generate_uuid(),
                'model': model or model_info['model'],
                'object': 'chat.completion.chunk',
                'choices': [{
                    'index': 1,
                    'delta': {
                        'role': 'assistant',
                        'content': f'ç”Ÿæˆå›¾ç‰‡å¤±è´¥: {str(e)}'
                    },
                    'finish_reason': 'stop'
                }]
            }
    except Exception as e:
        if retry_count < MAX_RETRY_COUNT:
            print(f"Response error: {str(e)}")
            print(f"Try again after {RETRY_DELAY / 1000}s...")
            time.sleep(RETRY_DELAY / 1000)
            async for chunk in create_completion_stream(messages, refresh_token, model, retry_count + 1):
                yield chunk
            return
        raise e 